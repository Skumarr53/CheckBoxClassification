{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5028dcb1",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847341a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T08:16:11.994916Z",
     "start_time": "2022-09-26T08:16:11.975601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3dd3977",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T08:16:13.508753Z",
     "start_time": "2022-09-26T08:16:12.127433Z"
    }
   },
   "outputs": [],
   "source": [
    "from boxdetect import config\n",
    "from boxdetect.img_proc import draw_rects, get_image\n",
    "from boxdetect.pipelines import get_boxes\n",
    "import matplotlib.pyplot as plt\n",
    "from pdb import set_trace\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5da24df7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T08:16:13.961037Z",
     "start_time": "2022-09-26T08:16:13.510237Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afe9ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T08:16:14.398397Z",
     "start_time": "2022-09-26T08:16:13.962312Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "# from pdb import set_trace\n",
    "# from collections import Counter\n",
    "import random\n",
    "import albumentations as A\n",
    "import imagesize\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b2ea19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T08:16:14.415078Z",
     "start_time": "2022-09-26T08:16:14.399967Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets import KMNIST\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08796b11",
   "metadata": {},
   "source": [
    "## Custom Funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdbf484b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T15:14:48.753114Z",
     "start_time": "2022-09-26T15:14:48.734419Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = config.PipelinesConfig()\n",
    "\n",
    "# important to adjust these values to match the size of boxes on your image\n",
    "cfg.width_range = (15,60)\n",
    "cfg.height_range = (15, 60)\n",
    "\n",
    "# the more scaling factors the more accurate the results but also it takes more time to processing\n",
    "# too small scaling factor may cause false positives\n",
    "# too big scaling factor will take a lot of processing time\n",
    "cfg.scaling_factors = [1.0]\n",
    "\n",
    "# w/h ratio range for boxes/rectangles filtering\n",
    "cfg.wh_ratio_range = (0.5, 2.5)\n",
    "\n",
    "# range of groups sizes to be returned\n",
    "cfg.group_size_range = (1, 30)\n",
    "\n",
    "# for this image we will use rectangles as a kernel for morphological transformations\n",
    "cfg.morph_kernels_type = 'rectangles'  # 'lines'\n",
    "\n",
    "# num of iterations when running dilation tranformation (to engance the image)\n",
    "cfg.dilation_iterations = 0\n",
    "\n",
    "\n",
    "# def get_CB_boundingboxes(file_path,cfg,out_dir=None):\n",
    "    \n",
    "#     try:\n",
    "#         image = cv2.imread(file_path)\n",
    "\n",
    "\n",
    "#         des_size = (1260, 1800)\n",
    "#         ori_size = image.shape[1],image.shape[0]\n",
    "\n",
    "#         ht_rat = des_size[1]/ori_size[1]\n",
    "#         wd_rat = des_size[0]/ori_size[0]\n",
    "\n",
    "#         resized_image = cv2.resize(image, des_size,interpolation = cv2.INTER_NEAREST)\n",
    "#         rects, grouping_rects, pimage, output_image = get_boxes(\n",
    "#             resized_image, cfg=cfg, plot=False)\n",
    "\n",
    "#         rects[:,[0,2]] = rects[:,[0,2]]/wd_rat\n",
    "#         rects[:,[1,3]] = rects[:,[1,3]]/ht_rat\n",
    "        \n",
    "#         if out_dir:\n",
    "#             out_img = draw_rects(get_image(file_path), rects, thickness=2)\n",
    "#             cv2.imwrite(f\"{out_dir}/{img}\", out_img)\n",
    "\n",
    "#         rects[:,2] = rects[:,2]+rects[:,0]\n",
    "#         rects[:,3] = rects[:,3]+rects[:,1]\n",
    "#     except:\n",
    "#         rects=np.array([])\n",
    "#     return image, rects\n",
    "\n",
    "# ## LENET: Model Architecture\n",
    "\n",
    "class LeNet(Module):\n",
    "    def __init__(self, numChannels, classes):\n",
    "        # call the parent constructor\n",
    "        super(LeNet, self).__init__()\n",
    "        # initialize first set of CONV => RELU => POOL layers\n",
    "        self.conv1 = Conv2d(in_channels=numChannels, out_channels=20,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize second set of CONV => RELU => POOL layers\n",
    "        self.conv2 = Conv2d(in_channels=20, out_channels=50,\n",
    "            kernel_size=(5, 5))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        # initialize first (and only) set of FC => RELU layers 64x450 and 800x500)\n",
    "        self.fc1 = Linear(in_features=450, out_features=500)\n",
    "        self.relu3 = ReLU()\n",
    "        # initialize our softmax classifier\n",
    "        self.fc2 = Linear(in_features=500, out_features=classes)\n",
    "        self.logSoftmax = LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # pass the input through our first set of CONV => RELU =>\n",
    "        # POOL layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        # pass the output from the previous layer through the second\n",
    "        # set of CONV => RELU => POOL layers\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        # flatten the output from the previous layer and pass it\n",
    "        # through our only set of FC => RELU layers\n",
    "        x = flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        # pass the output to our softmax classifier to get our output\n",
    "        # predictions\n",
    "        x = self.fc2(x)\n",
    "        output = self.logSoftmax(x)\n",
    "        # return the output predictions\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7563e93",
   "metadata": {},
   "source": [
    "## Create Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd4c041c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T08:16:16.495215Z",
     "start_time": "2022-09-26T08:16:16.480769Z"
    }
   },
   "outputs": [],
   "source": [
    "Data_dir = \"./CheckboxClassificationData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f42bbfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T08:16:16.829515Z",
     "start_time": "2022-09-26T08:16:16.792781Z"
    }
   },
   "outputs": [],
   "source": [
    "classes = []\n",
    "image_paths = []\n",
    "\n",
    "## collect image paths \n",
    "for sub_dir in glob.glob(Data_dir+'/*'):\n",
    "    cls = sub_dir.split('/')[-1]\n",
    "    for file in glob.glob(sub_dir+'/*'):\n",
    "        classes.append(cls)\n",
    "        image_paths.append(file)\n",
    "        \n",
    "        \n",
    "## Random shuffle \n",
    "img_lab_list = list(zip(image_paths, classes))\n",
    "random.shuffle(img_lab_list)\n",
    "\n",
    "train_img_lab_list, valid_img_lab_list = img_lab_list[:int(0.8*len(img_lab_list))], img_lab_list[int(0.8*len(img_lab_list)):] \n",
    "\n",
    "\n",
    "# train_image_paths, train_classes = zip(*train_img_lab_list)\n",
    "# valid_image_paths, valid_classes = zip(*valid_img_lab_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edefc047",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T08:16:18.047161Z",
     "start_time": "2022-09-26T08:16:17.396157Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_to_class = {'not_a_checkbox':0, 'not_selected':1, 'selected':2}\n",
    "class_to_idx = {y: x for x, y in idx_to_class.items()}\n",
    "\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(25,25),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, imglab_ziplist, transform=False):\n",
    "        self.image_paths,self.labels = zip(*imglab_ziplist)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.image_paths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #label = image_filepath.split('/')[-2]\n",
    "        label = idx_to_class[self.labels[idx]]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99395b12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:08:12.838266Z",
     "start_time": "2022-09-26T10:08:12.823480Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_img_lab_list,transforms)\n",
    "valid_dataset = CustomDataset(valid_img_lab_list,transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad11f41",
   "metadata": {},
   "source": [
    "## Model Tarining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85013adc",
   "metadata": {},
   "source": [
    "### Model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74fccf09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:08:13.848544Z",
     "start_time": "2022-09-26T10:08:13.703214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# define training hyperparameters\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb720139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:08:14.093169Z",
     "start_time": "2022-09-26T10:08:14.072325Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDataLoader = DataLoader(train_dataset, shuffle=True,batch_size=BATCH_SIZE)\n",
    "valDataLoader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "trainSteps = len(train_dataset) // BATCH_SIZE\n",
    "valSteps = len(train_dataset) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f6086ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:08:16.303517Z",
     "start_time": "2022-09-26T10:08:14.543638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    }
   ],
   "source": [
    "model = LeNet(\n",
    "    numChannels=3,\n",
    "    classes=len(idx_to_class)).to(device)\n",
    "\n",
    "# initialize our optimizer and loss function\n",
    "opt = Adam(model.parameters(), lr=INIT_LR)\n",
    "lossFn = nn.NLLLoss()\n",
    "\n",
    "# initialize a dictionary to store training history\n",
    "H = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "# measure how long training is going to take\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5787360",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:08:16.318442Z",
     "start_time": "2022-09-26T10:08:16.304786Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b427aba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:08:33.260830Z",
     "start_time": "2022-09-26T10:08:16.319348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/10\n",
      "Train loss: 0.206155, Train accuracy: 0.9380\n",
      "Val loss: 0.027941, Val accuracy: 0.9730\n",
      "\n",
      "[INFO] EPOCH: 2/10\n",
      "Train loss: 0.072177, Train accuracy: 0.9839\n",
      "Val loss: 0.013751, Val accuracy: 0.9890\n",
      "\n",
      "[INFO] EPOCH: 3/10\n",
      "Train loss: 0.049994, Train accuracy: 0.9882\n",
      "Val loss: 0.014093, Val accuracy: 0.9890\n",
      "\n",
      "[INFO] EPOCH: 4/10\n",
      "Train loss: 0.041544, Train accuracy: 0.9891\n",
      "Val loss: 0.010136, Val accuracy: 0.9920\n",
      "\n",
      "[INFO] EPOCH: 5/10\n",
      "Train loss: 0.037816, Train accuracy: 0.9911\n",
      "Val loss: 0.012797, Val accuracy: 0.9877\n",
      "\n",
      "[INFO] EPOCH: 6/10\n",
      "Train loss: 0.036024, Train accuracy: 0.9903\n",
      "Val loss: 0.015628, Val accuracy: 0.9847\n",
      "\n",
      "[INFO] EPOCH: 7/10\n",
      "Train loss: 0.031473, Train accuracy: 0.9920\n",
      "Val loss: 0.015209, Val accuracy: 0.9883\n",
      "\n",
      "[INFO] EPOCH: 8/10\n",
      "Train loss: 0.031539, Train accuracy: 0.9923\n",
      "Val loss: 0.010670, Val accuracy: 0.9914\n",
      "\n",
      "[INFO] EPOCH: 9/10\n",
      "Train loss: 0.030068, Train accuracy: 0.9920\n",
      "Val loss: 0.009800, Val accuracy: 0.9908\n",
      "\n",
      "[INFO] EPOCH: 10/10\n",
      "Train loss: 0.039406, Train accuracy: 0.9900\n",
      "Val loss: 0.013570, Val accuracy: 0.9890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PyTorch: Training your first Convolutional Neural Network (CNN)\n",
    "# loop over our epochs\n",
    "for e in range(0, EPOCHS):\n",
    "    \n",
    "    # set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # initialize the total training and validation loss\n",
    "    totalTrainLoss = 0\n",
    "    totalValLoss = 0\n",
    "    \n",
    "    # initialize the number of correct predictions in the training\n",
    "    # and validation step\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "    \n",
    "    # loop over the training set\n",
    "    for (x, y) in trainDataLoader:\n",
    "        # send the input to the device\n",
    "        (x, y) = (x.to(device), y.to(device))\n",
    "        # perform a forward pass and calculate the training loss\n",
    "        pred = model(x)\n",
    "        loss = lossFn(pred, y)\n",
    "        # zero out the gradients, perform the backpropagation step,\n",
    "        # and update the weights\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        # add the loss to the total training loss so far and\n",
    "        # calculate the number of correct predictions\n",
    "        totalTrainLoss += loss\n",
    "        trainCorrect += (pred.argmax(1) == y).type(\n",
    "            torch.float).sum().item()\n",
    "        \n",
    "    # switch off autograd for evaluation\n",
    "    with torch.no_grad():\n",
    "        # set the model in evaluation mode\n",
    "        model.eval()\n",
    "        # loop over the validation set\n",
    "        for (x, y) in valDataLoader:\n",
    "            # send the input to the device\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "            # make the predictions and calculate the validation loss\n",
    "            pred = model(x)\n",
    "            totalValLoss += lossFn(pred, y)\n",
    "            # calculate the number of correct predictions\n",
    "            valCorrect += (pred.argmax(1) == y).type(\n",
    "            torch.float).sum().item()\n",
    "            \n",
    "    # calculate the average training and validation loss\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgValLoss = totalValLoss / valSteps\n",
    "    # calculate the training and validation accuracy\n",
    "    trainCorrect = trainCorrect / len(trainDataLoader.dataset)\n",
    "    valCorrect = valCorrect / len(valDataLoader.dataset)\n",
    "    # update our training history\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"train_acc\"].append(trainCorrect)\n",
    "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "    H[\"val_acc\"].append(valCorrect)\n",
    "    # print the model training and validation information\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e + 1, EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Train accuracy: {:.4f}\".format(\n",
    "        avgTrainLoss, trainCorrect))\n",
    "    print(\"Val loss: {:.6f}, Val accuracy: {:.4f}\\n\".format(\n",
    "        avgValLoss, valCorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0a8bcb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:08:58.500960Z",
     "start_time": "2022-09-26T10:08:58.251947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] total time taken to train the model: 41.97s\n",
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92        58\n",
      "           1       0.99      1.00      0.99      1154\n",
      "           2       1.00      0.97      0.98       417\n",
      "\n",
      "    accuracy                           0.99      1629\n",
      "   macro avg       0.96      0.97      0.96      1629\n",
      "weighted avg       0.99      0.99      0.99      1629\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# finish measuring how long training took\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "    endTime - startTime))\n",
    "# we can now evaluate the network on the test set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "# turn off autograd for testing evaluation\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # initialize a list to store our predictions\n",
    "    preds = []\n",
    "    # loop over the test set\n",
    "    for (x, y) in valDataLoader:\n",
    "        # send the input to the device\n",
    "        x = x.to(device)\n",
    "        # make the predictions and add them to the list\n",
    "        pred = model(x)\n",
    "        preds.extend(pred.argmax(axis=1).cpu().numpy())\n",
    "# generate a classification report\n",
    "print(classification_report(np.vectorize(idx_to_class.get)(np.array(valid_dataset.labels)),np.array(preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ef6142b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-25T08:02:45.145989Z",
     "start_time": "2022-09-25T08:02:45.056805Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"CB_detection_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f5a67",
   "metadata": {},
   "source": [
    "## Check Box detction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "157370f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:10:51.434489Z",
     "start_time": "2022-09-26T10:10:51.392986Z"
    }
   },
   "outputs": [],
   "source": [
    "idx_to_class = {'not_a_checkbox':0, 'not_selected':1, 'selected':2}\n",
    "class_to_idx = {y: x for x, y in idx_to_class.items()}\n",
    "\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(25,25),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "device='cpu'\n",
    "# model = LeNet(\n",
    "#     numChannels=3,\n",
    "#     classes=len(idx_to_class)).to(device)\n",
    "# model.load_state_dict(torch.load(\"CB_detection_weights.pth\"))\n",
    "model = model.to(device) \n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8de4ae30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T15:15:05.810905Z",
     "start_time": "2022-09-26T15:15:05.791970Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_cb_prediction(file_path, cnn_transforms,model):\n",
    "    \n",
    "    class_to_idx = {0: 'not_a_checkbox', 1: 'not_selected', 2: 'selected'}\n",
    "\n",
    "    image,cb_boxes = get_CB_boundingboxes(file_path,cfg)\n",
    "    \n",
    "    cb_boxescounter = 0\n",
    "    \n",
    "    cb_items = {} \n",
    "    for bb in cb_boxes:\n",
    "        crop_img=image[bb[1]:bb[3], bb[0]:bb[2]]\n",
    "        crop_ten = cnn_transforms(image=crop_img)[\"image\"]\n",
    "        predict = model(crop_ten.unsqueeze_(0))\n",
    "        \n",
    "        pred_class = class_to_idx[predict.argmax(axis=1).item()]\n",
    "        \n",
    "        if pred_class != 'not_a_checkbox':\n",
    "            cb_items[f\"cb_{cb_boxescounter+1}\"] = [pred_class, bb]\n",
    "            cb_boxescounter+=1\n",
    "    return cb_items\n",
    "\n",
    "def get_CB_boundingboxes(file_path,cfg,out_dir=None):\n",
    "    \n",
    "    #try:\n",
    "\n",
    "    image = cv2.imread(file_path)\n",
    "\n",
    "\n",
    "    des_size = (1260, 1800)\n",
    "    ori_size = image.shape[1],image.shape[0]\n",
    "\n",
    "    ht_rat = des_size[1]/ori_size[1]\n",
    "    wd_rat = des_size[0]/ori_size[0]\n",
    "\n",
    "    resized_image = cv2.resize(image, des_size,interpolation = cv2.INTER_NEAREST)\n",
    "    rects, grouping_rects, pimage, output_image = get_boxes(\n",
    "        resized_image, cfg=cfg, plot=False)\n",
    "\n",
    "    rects[:,[0,2]] = rects[:,[0,2]]/wd_rat\n",
    "    rects[:,[1,3]] = rects[:,[1,3]]/ht_rat\n",
    "\n",
    "    if out_dir:\n",
    "        out_img = draw_rects(get_image(file_path), rects, thickness=2)\n",
    "        cv2.imwrite(f\"{out_dir}/{img}\", out_img)\n",
    "\n",
    "    rects[:,2] = rects[:,2]+rects[:,0]\n",
    "    rects[:,3] = rects[:,3]+rects[:,1]\n",
    "#     except:\n",
    "#         rects=np.array([])\n",
    "    return image, rects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b3843e",
   "metadata": {},
   "source": [
    "### Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "139d0950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T15:16:26.321779Z",
     "start_time": "2022-09-26T15:16:03.870740Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "testpath = 'ACORD_139_2004_03_p1_80_brit_page1.png'\n",
    "\n",
    "cb_items = get_cb_prediction(testpath, transforms, model)\n",
    "\n",
    "image = Image.open(testpath)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "    \n",
    "for k,v in cb_items.items():\n",
    "    draw.rectangle(list(v[1]), outline='green' if v[0]=='selected' else 'red', width=2)\n",
    "image.save(f\"Results/{testpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61e7e6f",
   "metadata": {},
   "source": [
    "### Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b4cf33e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:12:21.294390Z",
     "start_time": "2022-09-26T10:12:21.259975Z"
    }
   },
   "outputs": [],
   "source": [
    "testpath = 'ACORD_140_2016_03_p1_127_noi_page1.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "774552fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T10:12:30.916538Z",
     "start_time": "2022-09-26T10:12:22.627458Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cb_items = get_cb_prediction(testpath, transforms, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afad713b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T14:56:36.235689Z",
     "start_time": "2022-09-26T14:56:36.221154Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11ea7c75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T14:58:37.290986Z",
     "start_time": "2022-09-26T14:58:36.675860Z"
    }
   },
   "outputs": [],
   "source": [
    "image = Image.open(testpath)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "    \n",
    "for k,v in cb_items.items():\n",
    "    draw.rectangle(list(v[1]), outline='green' if v[0]=='selected' else 'red', width=2)\n",
    "image.save(f\"Results/{testpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee242d30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T14:56:52.781436Z",
     "start_time": "2022-09-26T14:56:52.766088Z"
    }
   },
   "source": [
    "### Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67475884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T15:03:09.950507Z",
     "start_time": "2022-09-26T15:03:01.215335Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "testpath = 'Acord_823_2011_10_p1_479_noi_brit_page1.png'\n",
    "\n",
    "cb_items = get_cb_prediction(testpath, transforms, model)\n",
    "\n",
    "image = Image.open(testpath)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "    \n",
    "for k,v in cb_items.items():\n",
    "    draw.rectangle(list(v[1]), outline='green' if v[0]=='selected' else 'red', width=2)\n",
    "image.save(f\"Results/{testpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f37faff",
   "metadata": {},
   "source": [
    "### Sample 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c9c9b15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-26T15:05:13.222612Z",
     "start_time": "2022-09-26T15:05:04.356682Z"
    }
   },
   "outputs": [],
   "source": [
    "testpath = 'Acord_129_p1_291_neg02rot_page1.png'\n",
    "\n",
    "cb_items = get_cb_prediction(testpath, transforms, model)\n",
    "\n",
    "image = Image.open(testpath)\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "    \n",
    "for k,v in cb_items.items():\n",
    "    draw.rectangle(list(v[1]), outline='green' if v[0]=='selected' else 'red', width=2)\n",
    "image.save(f\"Results/{testpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227953f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov7",
   "language": "python",
   "name": "yolov7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "645.455px",
    "left": "27px",
    "top": "111.052px",
    "width": "208.892px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
